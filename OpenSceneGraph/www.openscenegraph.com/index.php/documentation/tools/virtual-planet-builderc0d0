<?xml version="1.0" encoding="utf-8"?>
<!-- generator="Joomla! - Open Source Content Management" -->
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Virtual Planet Builder</title>
		<description><![CDATA[Project website for OpenSceneGraph]]></description>
		<link>https://www.openscenegraph.com/index.php/documentation/tools/virtual-planet-builder</link>
		<lastBuildDate>Thu, 30 Nov 2023 12:29:02 +0000</lastBuildDate>
		<generator>Joomla! - Open Source Content Management</generator>
		<atom:link rel="self" type="application/rss+xml" href="https://www.openscenegraph.com/index.php/documentation/tools/virtual-planet-builder?format=feed&amp;type=rss"/>
		<language>en-gb</language>
		<managingEditor>robert@openscenegraph.com (OpenSceneGraph)</managingEditor>
		<item>
			<title>Commandline options</title>
			<link>https://www.openscenegraph.com/index.php/documentation/tools/virtual-planet-builder/119-commandline-options</link>
			<guid isPermaLink="true">https://www.openscenegraph.com/index.php/documentation/tools/virtual-planet-builder/119-commandline-options</guid>
			<description><![CDATA[<h2 id="CommonCommandlineoptions">Common Commandline options</h2>
<table class="withborder">
<tbody>
<tr>
<td><strong>Parameter</strong></td>
<td><strong>Description</strong></td>
<td><strong>Default</strong></td>
</tr>
<tr>
<td><strong>General</strong></td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>-h or --help</td>
<td>Display commandline arguments information</td>
<td> </td>
</tr>
<tr>
<td>--task</td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>-s &lt;filename&gt;</td>
<td>Specify a VPB source file containing all commandline options.</td>
<td> </td>
</tr>
<tr>
<td>--so &lt;filename&gt;</td>
<td>Output the VPB source file for the current run.</td>
<td> </td>
</tr>
<tr>
<td>--report</td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--cache &lt;filename&gt;</td>
<td>Read the cache file to use a look up for locally cached files.</td>
<td> </td>
</tr>
<tr>
<td>--version</td>
<td>Print out version</td>
<td> </td>
</tr>
<tr>
<td>--version-number</td>
<td>Print out version number only.</td>
<td> </td>
</tr>
<tr>
<td>--comment</td>
<td>Added a comment/description string to the top most node in the dataset</td>
<td>empty</td>
</tr>
<tr>
<td>--split</td>
<td>Set the distributed build split level.</td>
<td> </td>
</tr>
<tr>
<td>--splits</td>
<td>Set the distributed build primary and secondary split levels.</td>
<td> </td>
</tr>
<tr>
<td>--run-path</td>
<td>Set the path that the build should be run from.</td>
<td> </td>
</tr>
<tr>
<td>--notify-level</td>
<td>Set the notify level when logging messages.</td>
<td> </td>
</tr>
<tr>
<td><strong>Input</strong></td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>-d &lt;filename&gt;</td>
<td>Specify the digital elevation map input file to process</td>
<td> </td>
</tr>
<tr>
<td>-t &lt;filename&gt;</td>
<td>Specify the texture map input file to process</td>
<td> </td>
</tr>
<tr>
<td>--building &lt;filename&gt;</td>
<td>Specify building outlines using shapefiles.</td>
<td> </td>
</tr>
<tr>
<td>--forest &lt;filename&gt;</td>
<td>Specify forest outlines using shapefiles</td>
<td> </td>
</tr>
<tr>
<td>--levels &lt;begin_level&gt; &lt;end_level&gt;</td>
<td>Specify the range of levels that the next source Texture or DEM will contribute to.</td>
<td> </td>
</tr>
<tr>
<td>--layer &lt;layer_num&gt;</td>
<td>Specify the layer that the next source Texture will contribute to..</td>
<td> </td>
</tr>
<tr>
<td><em>Coordinate system</em></td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--cs &lt;coordinates system string&gt;</td>
<td>Set the coordinates system of source imagery, DEM or destination database. The string may be any of the usual GDAL/OGR forms, complete WKT, PROJ.4, EPS</td>
<td> </td>
</tr>
<tr>
<td>--wkt &lt;WKT string&gt;</td>
<td>Set the coordinates system of source imagery, DEM or destination database in WellKownText form.</td>
<td> </td>
</tr>
<tr>
<td>--wkt-file &lt;WKT file&gt;</td>
<td>Set the coordinates system of source imagery, DEM or destination database by as file containing WellKownText definition.</td>
<td> </td>
</tr>
<tr>
<td><em>Geocentric database</em></td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--geocentric</td>
<td>Build a database in geocentric (i.e. whole earth) database.</td>
<td> </td>
</tr>
<tr>
<td>--bluemarble-east</td>
<td>Set the coordinates system for next texture or dem to represent the eastern hemisphere of the earth.</td>
<td> </td>
</tr>
<tr>
<td>--bluemarble-west</td>
<td>Set the coordinates system for next texture or dem to represent the western hemisphere of the earth.</td>
<td> </td>
</tr>
<tr>
<td>--whole-globe</td>
<td>Set the coordinates system for next texture or dem to represent the whole hemisphere of the earth.</td>
<td> </td>
</tr>
<tr>
<td><em>Ellipsoid model</em></td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--radius-polar</td>
<td>Set the polar radius of the ellipsoid model when building a geocentric database.</td>
<td>6356752.3142</td>
</tr>
<tr>
<td>--radius-equator</td>
<td>Set the equatorial radius of the ellipsoid model when building a geocentric database.</td>
<td>6378137</td>
</tr>
<tr>
<td>--spherical</td>
<td>Set the polar and equatorial radius both to the average of the two.</td>
<td> </td>
</tr>
<tr>
<td><em>Flat database</em></td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--range &lt;xMin&gt; &lt;xMax&gt; &lt;yMin&gt; &lt;yMax&gt;</td>
<td>Set the coordinates system for next texture or dem to the given range.</td>
<td> </td>
</tr>
<tr>
<td>--xx</td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--xt</td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--yy</td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--yt</td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--zz</td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--zt</td>
<td> </td>
<td> </td>
</tr>
<tr>
<td> </td>
<td> </td>
<td> </td>
</tr>
<tr>
<td><em>DataType</em></td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--vector</td>
<td>Interpret input as a vector data set</td>
<td> </td>
</tr>
<tr>
<td>--raster</td>
<td>Interpret input as a raster data set (default)</td>
<td> </td>
</tr>
<tr>
<td> </td>
<td> </td>
<td> </td>
</tr>
<tr>
<td><strong>Output</strong></td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--image-ext &lt;ext&gt;</td>
<td>Specify the Image format to output to via its plugin name, i.e. rgb, dds, jp2, jpeg.</td>
<td>.dds</td>
</tr>
<tr>
<td>-o &lt;outputfile&gt;</td>
<td>Specify the output master file to generate</td>
<td> </td>
</tr>
<tr>
<td>-a &lt;archivename&gt;</td>
<td>Specify the archive to place the generated database</td>
<td> </td>
</tr>
<tr>
<td>--ibn &lt;buildname&gt;</td>
<td>Specify the intermediate build file name</td>
<td> </td>
</tr>
<tr>
<td>-l &lt;numOfLevels&gt;</td>
<td>Specify the number of PagedLOD levels to generate</td>
<td>30</td>
</tr>
<tr>
<td>-e &lt;x&gt; &lt;y&gt; &lt;w&gt; &lt;h&gt;</td>
<td>Extents of the model to generate</td>
<td> </td>
</tr>
<tr>
<td>-ge &lt;x&gt; &lt;y&gt; &lt;w&gt; &lt;h&gt;</td>
<td>Geographic (Lat/Lon) Extents of the model to generate.</td>
<td> </td>
</tr>
<tr>
<td>-b &lt;xa&gt; &lt;ya&gt; &lt;xb&gt; &lt;yb&gt;</td>
<td>Bounds (similar to extents) of the model to generate. Max/Min order is not important.</td>
<td> </td>
</tr>
<tr>
<td>-gb &lt;xa&gt; &lt;ya&gt; &lt;xb&gt; &lt;yb&gt;</td>
<td>Geographic Bounds (similar to extents) of the model to generate. Max/Min order is not important.</td>
<td> </td>
</tr>
<tr>
<td>--skirt-ratio &lt;float&gt;</td>
<td>Set the ratio of skirt height to tile size</td>
<td>0.02</td>
</tr>
<tr>
<td>-v</td>
<td>Set the vertical multiplier</td>
<td>1.0</td>
</tr>
<tr>
<td>--no-terrain-simplification</td>
<td>Switch off terrain simplification.</td>
<td>true</td>
</tr>
<tr>
<td>--default-color &lt;r,g,b,a&gt;</td>
<td>Sets the default color of the terrain.</td>
<td>(0.5, 0.5, 1.0, 1.0)</td>
</tr>
<tr>
<td>--radius-to-max-visible-distance-ratio</td>
<td>Set the maximum visible distance ratio for all tiles apart from the top most tile. The maximum visible distance is computed from the ratio * tile radius.</td>
<td>7.0</td>
</tr>
<tr>
<td>--max-anisotropy</td>
<td>Max anisotropy level to use when texturing</td>
<td>1.0</td>
</tr>
<tr>
<td><em><a class="missing wiki" href="http://www.openscenegraph.org/projects/VirtualPlanetBuilder/wiki/GeometryType" rel="nofollow">GeometryType?</a></em></td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--HEIGHT_FIELD</td>
<td>Create a height field database</td>
<td> </td>
</tr>
<tr>
<td>--POLYGONAL</td>
<td>Create a height field database (default)</td>
<td> </td>
</tr>
<tr>
<td>--TERRAIN</td>
<td>Create a osgTerrain::Terrain database</td>
<td> </td>
</tr>
<tr>
<td> </td>
<td> </td>
<td> </td>
</tr>
<tr>
<td><em><a class="missing wiki" href="http://www.openscenegraph.org/projects/VirtualPlanetBuilder/wiki/DatabaseType" rel="nofollow">DatabaseType?</a></em></td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--LOD</td>
<td>Create a LOD'd database</td>
<td> </td>
</tr>
<tr>
<td>--PagedLOD</td>
<td>Create a PagedLOD'd database (default)</td>
<td> </td>
</tr>
<tr>
<td> </td>
<td> </td>
<td> </td>
</tr>
<tr>
<td><em><span class="missing wiki">TextureType</span></em></td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--compressed</td>
<td>Use OpenGL compression on RGB destination imagery (default)</td>
<td> </td>
</tr>
<tr>
<td>--RGBA-compressed</td>
<td>Use OpenGL compression on RGBA destination imagery</td>
<td> </td>
</tr>
<tr>
<td>--RGB-16</td>
<td>Use 16bit RGB destination imagery</td>
<td> </td>
</tr>
<tr>
<td>--RGB-24</td>
<td>Use 24bit RGB destination imagery</td>
<td> </td>
</tr>
<tr>
<td>--RGBA-16</td>
<td>Use 16bit RGBA destination imagery</td>
<td> </td>
</tr>
<tr>
<td>--RGBA</td>
<td>Use 32bit RGBA destination imagery</td>
<td> </td>
</tr>
<tr>
<td> </td>
<td> </td>
<td> </td>
</tr>
<tr>
<td><em>MipMappingMode</em></td>
<td> </td>
</tr>
<tr>
<td>--no-mip-mapping</td>
<td>Disable mip mapping of textures</td>
<td> </td>
</tr>
<tr>
<td>--mip-mapping-hardware</td>
<td>Use mip mapped textures, and generate the mipmaps in hardware when available.</td>
<td> </td>
</tr>
<tr>
<td>--mip-mapping-imagery</td>
<td>Use mip mapped textures, and generate the mipmaps in imagery. (default)</td>
<td> </td>
</tr>
<tr>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--BuildOverlays [True/False]</td>
<td>Switch on/off the building of overlay within the source imagery. Overlays can help reduce texture aliasing artifacts.</td>
<td>false</td>
</tr>
<tr>
<td>--ReprojectSources [True/False]</td>
<td>Switch on/off the reprojection of any source imagery that aren't in the correct projection for the database build.</td>
<td>true</td>
</tr>
<tr>
<td>--GenerateTiles [True/False]</td>
<td>Switch on/off the generation of the output database tiles.</td>
<td>true</td>
</tr>
<tr>
<td>--tile-image-size</td>
<td>Set the tile maximum image size</td>
<td>256</td>
</tr>
<tr>
<td>--tile-terrain-size</td>
<td>Set the tile maximum terrain size</td>
<td>64</td>
</tr>
<tr>
<td>-O</td>
<td>string option to pass to write plugins, use "" for multiple options</td>
<td> </td>
</tr>
<tr>
<td>--subtile &lt;LOD&gt; &lt;X&gt; &lt;Y&gt;</td>
<td>Set the subtile to begin the build from.</td>
<td> </td>
</tr>
<tr>
<td>--record-subtile-on-leaf-tiles</td>
<td>Enable the setting of the subtile file name of the leaf tiles.</td>
<td>false</td>
</tr>
<tr>
<td>--type-attribute</td>
<td>Set the type name which specify how the shapes should be interpreted in shapefile/dbase files. (empty signifies no type attribute has been defined)</td>
<td>NAME</td>
</tr>
<tr>
<td>--height-attribute</td>
<td>Set the attribute name for height attributes used in shapefile/dbase files.</td>
<td>HGT</td>
</tr>
<tr>
<td>--height</td>
<td>Set the height to use for associated shapefiles. (negative signifies that no height has been defined)</td>
<td>-1.0</td>
</tr>
<tr>
<td>--mask</td>
<td>Set the mask to assign indivual shapefile/model.</td>
<td>0xffffffff</td>
</tr>
<tr>
<td>--terrain-mask</td>
<td>Set the overall mask to assign terrain.</td>
<td>0xffffffff</td>
</tr>
<tr>
<td>--read-threads-ratio &lt;ratio&gt;</td>
<td>Set the ratio number of read threads relative to number of cores to use.</td>
<td>0.0</td>
</tr>
<tr>
<td>--write-threads-ratio &lt;ratio&gt;</td>
<td>Set the ratio number of write threads relative to number of cores to use.</td>
<td>0.0</td>
</tr>
<tr>
<td>--build-options &lt;string&gt;</td>
<td>Set build options string.</td>
<td> </td>
</tr>
<tr>
<td><em>DEM interpolation</em></td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--interpolate-terrain</td>
<td>Enable the use of interpolation when sampling data from source DEMs. (default)</td>
<td> </td>
</tr>
<tr>
<td>--no-interpolate-terrain</td>
<td>Disable the use of interpolation when sampling data from source DEMs.</td>
<td> </td>
</tr>
<tr>
<td><em>Imagery interpolation</em></td>
<td> </td>
<td> </td>
</tr>
<tr>
<td>--interpolate-imagery</td>
<td>Enable the use of interpolation when sampling data from source imagery. (default)</td>
<td> </td>
</tr>
<tr>
<td>--no-interpolate-imagery</td>
<td>Disable the use of interpolation when sampling data from source imagery.</td>
<td> </td>
</tr>
</tbody>
</table>]]></description>
			<author>jtorresfabra@gmail.com (openscenegraph)</author>
			<category>Virtual Planet Builder</category>
			<pubDate>Fri, 15 Feb 2013 01:56:19 +0000</pubDate>
		</item>
		<item>
			<title>How to compress an existing database</title>
			<link>https://www.openscenegraph.com/index.php/documentation/tools/virtual-planet-builder/118-how-to-compress-an-existing-database</link>
			<guid isPermaLink="true">https://www.openscenegraph.com/index.php/documentation/tools/virtual-planet-builder/118-how-to-compress-an-existing-database</guid>
			<description><![CDATA[<div id="content" class="wiki">
<div class="wikipage searchable">
<p id="Howtocompressanexistingdatabase">Versions of OSG since around November 2008 support the compression of .ive files using the -O "compressed" writer option. I've tested the procedure on a 22GB database and the resulting database was reduced to 13GB.</p>
<h3 id="Overview">Overview</h3>
<p>The idea behind this example is to take an existing database that was not built using compression and use <strong>osgconv</strong> to compress each .ive file into a new database. This procedure is not needed if one creates the database with compression from the start by passing -O "compressed" to vpbmaster.</p>
<h3 id="Assumptions">Assumptions</h3>
<p>* Let <strong>old_db</strong> = the root of the old database, i.e. the directory where the top-level .ive file is located.</p>
<p>* Assume that the parent dir of <strong>old_db</strong> is writable.</p>
<p>* Let <strong>new_db</strong> = the root of the new database. It would be a sibling of the <strong>old_db</strong> directory.</p>
<p>* Using bash on Linux.</p>
<h3 id="Procedure">Procedure</h3>
<p>Turn off optimization</p>
<pre class="wiki">export OSG_OPTIMIZER=OFF
</pre>
<p>Create a script to convert all .ive files in a given directory (reason later)</p>
<pre class="wiki">cd old_db
emacs conv_all_in_dir.sh
</pre>
<p>Put the following in the script, edit as needed, comments inline</p>
<pre class="wiki">#!/bin/bash

# used with e.g.:
#find . -type d -exec ./conv_all_in_dir.sh {} \;

# the output root, edit as needed
OUT_ROOT=`pwd`/../new_db
#echo $OUT_ROOT

# 1st parameter is the directory we should process
IN_DIR=$1;
echo Changing to $IN_DIR
cd $IN_DIR

# find: find all ive files in this directory only, not subdirs
# sed: remove ./ in front of names
# xargs: run for every input file, start multiple processes (-P)
# osgconv -O "compressed": make compressed ive files

find . -maxdepth 1 -name "*.ive" | sed -e s/"\.\/"// | xargs -P 2 -I {} osgconv {} $OUT_ROOT/$IN_DIR/{} -O "compressed"

</pre>
<p>The script is passed a subdirectory of <strong>old_db</strong>, it goes through that directory and converts all the .ive files and places them in the correct <strong>new_db</strong> directory.</p>
<p>If you have a multi-core machine, adjust the -P parameter of xargs to suit.</p>
<p>Make the script executable</p>
<pre class="wiki">chmod u+x ./conv_all_in_dir.sh
</pre>
<p>Recreate the directory structure of the input database. <strong>osgconv</strong> does not like to create directories.</p>
<pre class="wiki">find . -type d -exec mkdir -p ../new_db/{} \;
</pre>
<p>Now, go to each directory of old_db and convert all the .ive files into the proper directory of new_db.</p>
<pre class="wiki">find . -type d -exec ./conv_all_in_dir.sh {} \;
</pre>
<p>Wait...</p>
<h2 id="Reasonforcdbeforeosgconv">Reason for cd before osgconv</h2>
<p>For some reason <strong>osgconv</strong> must be run with only the input file name, i.e. from the directory where the file is located. If it is run by passing not only the filename, but a whole path to the file, the links to files in other directories get corrupted somehow. When the links are broken, one can only open the database properly from a single directory, e.g.</p>
<pre class="wiki">cd new_db
osgviewer terrain.ive
</pre>
<p>would work, but</p>
<pre class="wiki">cd ..
osgviewer new_db/terrain.ive
</pre>
<p>or execution from any other path would not.</p>
</div>
</div>]]></description>
			<author>jtorresfabra@gmail.com (openscenengraph)</author>
			<category>Virtual Planet Builder</category>
			<pubDate>Fri, 15 Feb 2013 01:45:52 +0000</pubDate>
		</item>
		<item>
			<title>How to continue a canceled or failed vpbmaster rendering</title>
			<link>https://www.openscenegraph.com/index.php/documentation/tools/virtual-planet-builder/122-how-to-continue-a-canceled-or-failed-vpbmaster-rendering</link>
			<guid isPermaLink="true">https://www.openscenegraph.com/index.php/documentation/tools/virtual-planet-builder/122-how-to-continue-a-canceled-or-failed-vpbmaster-rendering</guid>
			<description><![CDATA[<p>Rendering large databases (0,7 TB raw data and more) often causes the operating system or any part of it to crash. While using 64 OS, the limit is pushed away, it is not solved.</p>
<h3 id="Virtualrenderingsetup">Virtual rendering setup</h3>
<p>Let us assume a virtual renderering setup:</p>
<ul>
<li>You are using Kubuntu 9.10 64bit</li>
<li>You are using OSG 2.9.5 and VPB 0.9.11</li>
<li>Your working directory:
<pre class="wiki">/myWorkingDirectory
</pre>
</li>
<li>Your output diretory is:
<pre class="wiki">/myWorkingDirectory/output
</pre>
</li>
<li>Your sourcedata (dem and texture) resides in:
<pre class="wiki">/myWorkingDirectory/source/dem/
/myWorkingDirectory/source/orthophoto/
</pre>
</li>
<li>Your machinepool.txt is stored in your working directory and contains:
<pre class="wiki">Machine {
        hostname localhost
        processes 8
}

</pre>
</li>
<li>Your compilescript "compile.sh" is stored in your working directory and contains:
<pre class="wiki">#!/bin/sh

vpbmaster --machines machinepool.txt\
 --notify-level ALWAYS\
 --geocentric\
 --terrain\
 --compressed\
 -d source/dem \
 -t source/orthophoto \
 -o output/terrain.ive
</pre>
</li>
</ul>
<h3 id="Compileadataset">Compile a dataset</h3>
<p>Now, to compile your dataset using this script, type:</p>
<pre class="wiki">cd /myWorkingDirectory
./compile.sh
</pre>
<p>The vpbmaster now runs as usual, and creates the following folder an files:</p>
<ul>
<li>logs - this directory contains the logfiles for each submitted task - a good place to search for the reasons why your tasks failed :).</li>
<li>tasks - This directory contains all taskfiles with there status (pending or completed).</li>
<li>build_master.source -This file contains all information you passed to vpbmaster via the commandline options.</li>
<li>build_master.tasks - This file contains a list of all taskfiles in this compile project.</li>
</ul>
<p>If your run is canceled, or due to OS reasons your tasks fail, you can restart your build process in three alternative ways:</p>
<ul>
<li>Just restart your compile.sh script - <strong><em>THIS WILL OVERWRITE ALL ALREADY COMPILED DATA! </em></strong> Or...</li>
<li>Just restart vpbmaster with the created build_master.source via <tt>vpbmaster -s build_master.source</tt>. - <strong><em>THIS WILL OVERWRITE ALL ALREADY COMPILED DATA TOO! </em></strong> Therefore....</li>
<li><span class="underline">Resume your Build</span>...</li>
</ul>
<h3 id="Resumingabuildprocess">Resuming a build process</h3>
<p>To resume a build process, just go to your working directory and call vpbmaster with your specified task file list:</p>
<pre class="wiki">cd /myWorkingDirectory
vpbmaster --tasks build_master.tasks
</pre>
<p>Now vpbmaster does NOT build up a new tasklist and new taskfiles, but he just uses existing taskfiles and skips all tasks which are already finished.</p>]]></description>
			<author>jtorresfabra@gmail.com (Torben Dannhauer)</author>
			<category>Virtual Planet Builder</category>
			<pubDate>Fri, 15 Feb 2013 02:25:49 +0000</pubDate>
		</item>
		<item>
			<title>How to extract dataset parts</title>
			<link>https://www.openscenegraph.com/index.php/documentation/tools/virtual-planet-builder/117-how-to-extract-dataset-parts</link>
			<guid isPermaLink="true">https://www.openscenegraph.com/index.php/documentation/tools/virtual-planet-builder/117-how-to-extract-dataset-parts</guid>
			<description><![CDATA[<h3 id="SupportedOSGVersions">Supported OSG Versions</h3>
<p>Versions of OSG since February 10th 2012 (&gt;3.0.1) support the caching of OSG models from local sources. This is required to follow this tutorial.</p>
<h3 id="Overview">Overview</h3>
<p>The idea behind this example is to take an existing database that was built with VPB and use <strong>osgfilecache</strong> to 'cache' the local terrain database to another local directory. osgfilecache supports level/extends limitation, so it is possible to extract only terrain within certain bounds or level of detail.</p>
<h3 id="Assumptions">Assumptions</h3>
<ul>
<li> Let <strong>/home/large_database</strong> = the root of the original (large) database, i.e. the directory where the top-level .osgb file is located.</li>
<li>Let <strong>/home/database_part</strong> = the root of the destination database with limited extends. It should be writable to allow osgfilecache to write the files into it.</li>
<li>The large source database root was called terrain.osgb (<strong>/home/large_database/terrain.osgb</strong>)</li>
<li>The large source database was build in <strong>geocentric</strong> mode.</li>
<li>Using bash on Linux. Replacing the pathes it should be possible to follow the recipe also on windows. <strong>Be aware of he limited NTFS performance and stability if you try to extract giga-/terabyte size datasets''' </strong></li>
</ul>
<h3 id="Procedure">Procedure</h3>
<p>One note before we start with the commands: pagedLOD nodes store the database path, which is the path to the file. This path is overwritten by osgfilecache. To get a independent new sub database with relative paths, you mast enter the directory of the large source terrain database before you invoke osgfilecache. Otherwise you will get a terrain dataset with absolute paths which will cause OSG to continue loading from the original database instead from the extracted limited dataset.</p>
<p>Enter the source terrain dataset</p>
<pre class="wiki">cd /home/large_database
</pre>
<p>The command structure is 'osgfilecache --file-cache &lt;your destination folder&gt; &lt;options to control the new database extents&gt; &lt;relative path to your source database root file&gt; For example for the above assumptions, it would be the following to extract the database until LOD level 3 ('-l 3'):</p>
<pre class="wiki">osgfilecache --file-cache /home/database_part -l 3 ./terrain.osgb
</pre>
<p>According to the osgfielcache help, the following extend definitions are allowed:</p>
<pre class="wiki"> -e level minX minY maxX maxY                                                  
                    Read down to &lt;level&gt; across the extents minX, minY to maxY,
                    maxY.  Note, for geocentric datase X and Y are longitude and
                    latitude respectively.
  -l level          Read down to level across the whole database.

</pre>
<h3 id="Example">Example</h3>
<p>To extract lon 11-15 and lat 30-40 until level 8:</p>
<pre class="wiki">osgfilecache --file-cache /home/torben/test -e 8 11 30 15 40 ./terrain.osgb
</pre>
<h3 id="Advancedusage">Advanced usage</h3>
<p><strong>Note: You can run osgfilecache multiple times in the same directory to 'compose' a new database with different extends. E.g. worldwide&lt;=level 5, europe&lt;=level 8 and germany&lt;=level 25</strong></p>]]></description>
			<author>jtorresfabra@gmail.com (Torben Dannhauer)</author>
			<category>Virtual Planet Builder</category>
			<pubDate>Thu, 14 Feb 2013 20:56:17 +0000</pubDate>
		</item>
		<item>
			<title>How to patch an existing database</title>
			<link>https://www.openscenegraph.com/index.php/documentation/tools/virtual-planet-builder/121-how-to-patch-an-existing-database</link>
			<guid isPermaLink="true">https://www.openscenegraph.com/index.php/documentation/tools/virtual-planet-builder/121-how-to-patch-an-existing-database</guid>
			<description><![CDATA[<div id="content" class="wiki">
<div class="wikipage searchable">
<p id="Howtopatchanexistingdatabase">Versions of VPB since around June 2009 support patching of existing .ive database using the --patch option. This options has not been commented right now but we infer it from the source code. We've tested the procedure on different cases and it seems to work fine.</p>
<h3 id="Overview">Overview</h3>
<p>The idea behind this example is to take an existing database and add raster/elevation data to update it with patches of higher resolution.</p>
<h3 id="Assumptions">Assumptions</h3>
<ul>
<li>Let <strong>old_db.ive</strong> = the root of the old database, i.e. the path where the top-level .ive file is located.</li>
<li>Let <strong>new_db_raster.tif</strong> = a new raster data to be added to the old database.</li>
<li>Let <strong>new_db_dem.tif</strong> = a new elevation data to be added to the old database.</li>
</ul>
<h3 id="Procedure">Procedure</h3>
<p>Use the following command line :</p>
<pre class="wiki">vpbmaster^
 --patch old_db.ive^
 --levels 12 16 -t new_db_raster.tif^
 --levels 12 16 -d new_db_dem.tif
</pre>
<p>If you don't specify the levels where the patch should appear, then it will always be visible on top of the underlying database. Other options will be kept from the old database construction by VPB.</p>
<h3 id="Remarks">Remarks</h3>
<p>For some reason the original source files used to produce the old database should be kept along with it while patching, otherwise VPB will claim about missing files. However, if you provide VPB with relative file names at construction it seems that you can move the original files and the database together.</p>
<p>We also recommand to create a root directory containing the database. Indeed, VPB will create revision files (.source, .revision, .added) along with the root ive files. These files must be kept in order to patch the database afterwards.</p>
<pre class="wiki">vpbmaster^
 -t world_A1.tif^
 -t world_A2.tif^
 -t world_A3.tif^
 -t world_A4.tif^
 -d gtopo90.tif^
 --mip-mapping-hardware^
 --geocentric^
 --terrain^
 --RGB-24^
 -l 16^
 -O "compressImageData JPEG_QUALITY 75"^
 -o World450m/world450m.ive
</pre>
<p>Here you will be able to move the tif files and the World450m root directory together to another location for patching.</p>
</div>
</div>]]></description>
			<author>jtorresfabra@gmail.com (openscenegraph)</author>
			<category>Virtual Planet Builder</category>
			<pubDate>Fri, 15 Feb 2013 02:15:20 +0000</pubDate>
		</item>
		<item>
			<title>VirtualPlanetBuilder on a Single System Image (SSI) Cluster Example</title>
			<link>https://www.openscenegraph.com/index.php/documentation/tools/virtual-planet-builder/120-virtualplanetbuilder-on-a-single-system-image-ssi-cluster-example</link>
			<guid isPermaLink="true">https://www.openscenegraph.com/index.php/documentation/tools/virtual-planet-builder/120-virtualplanetbuilder-on-a-single-system-image-ssi-cluster-example</guid>
			<description><![CDATA[<p id="VirtualPlanetBuilderonaSingleSystemImageSSIClusterExample">This page briefly describes the setup and procedure used to generated a large terrain database on a cluster of 8 nodes. This is definitely not the only way to use VirtualPlanetBuilder, but serves as a specific example.</p>
<p>I used OpenSceneGraph (rev 8413) and VirtualPlanetBuilder (rev 914) checked out from svn around June 2008.</p>
<h3>Prerequisites</h3>
<p>This section describes some aspects of the cluster setup that are related to running VirtualPlanetBuilder.</p>
<h4 id="PasswordlessSSH">Passwordless SSH</h4>
<p>VirtualPlanetBuilder by default uses <strong>ssh</strong> to execute <strong>osgdem</strong> commands on the compute nodes. You should therefore setup <strong>ssh</strong> so that the user that will run the <strong>vpbmaster</strong> command can login to the other machines without a password. I used public/private key pairs to do this. Consult Google for detailed instructions.</p>
<p>If all is setup correctly, you should be able to login from the node where you will run <strong>vpbmaster</strong> to any other node without a password, e.g.:</p>
<pre class="wiki">jpd@rootnode:~$ ssh node01
Linux node01 2.6.24 #1 SMP Wed Jul 9 16:58:57 SAST 2008 i686
Last login: Sat Jul 19 11:52:59 2008 from rootnode
jpd@node01:~$
</pre>
<h4 id="XServeronNodes">X Server on Nodes</h4>
<p>When <strong>osgdem</strong> executes on the compute nodes, it tries to open a window on display <strong>:0.0</strong>. An X server must therefore be running on the node (the first server on most Linux distros should default to :0.0). It is important to log in to the server. All the nodes in this example had an NVidia graphics card with the 169.12 driver installed.</p>
<p>To test if everything is working correctly, do something like the following:</p>
<pre class="wiki">jpd@rootnode:~$ ssh node01 "export DISPLAY=:0.0 ; xeyes"
</pre>
<p>The <strong>xeyes</strong> application should run and display its window on node01's X server.</p>
<h4 id="DataDirectories">Data Directories</h4>
<p>All nodes that will participate in the terrain building need to have access to the input data (readable) as well as access to a directory to store the output files (writable). In this example I will assume that a directory called "/glusterfs" is visible from all machines and is writable.</p>
<p>The following should for example display the same listing:</p>
<pre class="wiki">jpd@rootnode:~$ ls /glusterfs
jpd@rootnode:~$ ssh node05 "ls /glusterfs"
</pre>
<h3 id="VPBSetup">VPB Setup</h3>
<h4 id="DataReprojectionandTranslation">Data Reprojection and Translation</h4>
<p>VirtualPlanetBuilder seems happiest when all input data uses the same projection and datum. It is also a good idea to reproject the input data so that it can be used in future VirtualPlanetBuilder runs (yes, you will run it more than once ;).</p>
<p>For this example I used latlong and WGS84 for all the files and also converted everything to GeoTiff format. The following shows how I converted the stunning Blue Marble Next Generation (BMNG) data I downloaded from <a class="ext-link" href="http://mirrors.arsc.edu/nasa/world_500m/"><span class="icon">here</span></a>.</p>
<pre class="wiki">gdal_translate -of GTiff -a_srs "+proj=latlong +datum=WGS84" -a_ullr -180 90 -90   0 world.topo.bathy.200407.3x21600x21600.A1.jpg A1.tif
gdal_translate -of GTiff -a_srs "+proj=latlong +datum=WGS84" -a_ullr -90  90   0   0 world.topo.bathy.200407.3x21600x21600.B1.jpg B1.tif
gdal_translate -of GTiff -a_srs "+proj=latlong +datum=WGS84" -a_ullr   0  90  90   0 world.topo.bathy.200407.3x21600x21600.C1.jpg C1.tif
gdal_translate -of GTiff -a_srs "+proj=latlong +datum=WGS84" -a_ullr  90  90 180   0 world.topo.bathy.200407.3x21600x21600.D1.jpg D1.tif
gdal_translate -of GTiff -a_srs "+proj=latlong +datum=WGS84" -a_ullr -180  0 -90 -90 world.topo.bathy.200407.3x21600x21600.A2.jpg A2.tif
gdal_translate -of GTiff -a_srs "+proj=latlong +datum=WGS84" -a_ullr  -90  0   0 -90 world.topo.bathy.200407.3x21600x21600.B2.jpg B2.tif
gdal_translate -of GTiff -a_srs "+proj=latlong +datum=WGS84" -a_ullr    0  0  90 -90 world.topo.bathy.200407.3x21600x21600.C2.jpg C2.tif
gdal_translate -of GTiff -a_srs "+proj=latlong +datum=WGS84" -a_ullr   90  0 180 -90 world.topo.bathy.200407.3x21600x21600.D2.jpg D2.tif
</pre>
<p>For most data that already has projection data included, the following command seemed to work:</p>
<pre class="wiki">gdalwarp -t_srs "+proj=latlong +datum=WGS84" -r bilinear $name ../reprojected/$newname
</pre>
<p>I usually place all reprojected data into separate directories that group them by layer. E.g. the converted BMNG tif files would be placed in a directory called "/glusterfs/BMNG".</p>
<h4 id="MachinePoolFile">Machine Pool File</h4>
<p>VirtualPlanetBuilder uses an input file to describe the list of machines that will be used during the build. Below is the simple one I used, called <strong>machinepool.txt</strong>.</p>
<pre class="wiki">Machine {
	hostname node01
	processes 1
}
Machine {
	hostname node02
	processes 1
}
Machine {
	hostname node03
	processes 1
}
Machine {
	hostname node04
	processes 1
}
Machine {
	hostname node05
	processes 1
}
Machine {
	hostname node06
	processes 1
}
Machine {
	hostname node07
	processes 1
}
Machine {
	hostname node08
	processes 1
}
</pre>
<p>The names specified in the file should be the names that are reachable using <strong>ssh</strong>. If you have multiple cores per machine with enough memory, you could try increasing the processes.</p>
<h3 id="CommandLine">Command Line</h3>
<p>Below is the command line that I used (all should be on one line when executing without the // comments):</p>
<pre class="wiki">vpbmaster --machines machinepool.txt                // Use the machine pool file described earlier
          --geocentric                              // I want a geocentric database
          -d /glusterfs/DTED                        // My elevation data can be found here (the directory contains only tifs)
          --layer 0 -t /glusterfs/BMNG              // On texture layer 0 I want the BMNG data
          --layer 0 -t /glusterfs/SPOT_reproj       // as well as some nice satellite photos
          --layer 1 -t /glusterfs/Maps_reproj       // On Layer 1 I want some 1:50000 scale maps
          --terrain --compressed                    // Use the new terrain format and compress the texture data
          -o spot_maps/terrain.ive                  // Put the output files under this directory
</pre>
<p>The command was executed from the master node in a directory that is visible to all nodes:</p>
<pre class="wiki">jpd@rootnode:~$ cd /glusterfs/generate
</pre>
<p>That's it. The <strong>vpbmaster</strong> command created 473 tasks and after 50 hours of processing created 1.5 million files with a total size of 487GB.</p>
<p>Below is a snippet of the output:</p>
<pre class="wiki">End of run: tasksPending=0 taskCompleted=473 taskRunning=0 tasksFailed=0
MachinePool::reportTimingStats()
    Machine : node01
        Task::type=''   minTime=616.267390      maxTime=5068.351865     averageTime=2751.044383 totalComputeTime=181568.929248  numTasks=66
    Machine : node02
        Task::type=''   minTime=761.555931      maxTime=25294.378808    averageTime=4027.837882 totalComputeTime=181252.704694  numTasks=45
    Machine : node03
        Task::type=''   minTime=702.828047      maxTime=4933.171209     averageTime=2936.182243 totalComputeTime=179107.116819  numTasks=61
    Machine : node04
        Task::type=''   minTime=605.509313      maxTime=5256.497770     averageTime=2794.609616 totalComputeTime=178855.015440  numTasks=64
    Machine : node05
        Task::type=''   minTime=704.703147      maxTime=5562.438701     averageTime=3005.987053 totalComputeTime=180359.223195  numTasks=60
    Machine : node06
        Task::type=''   minTime=658.961472      maxTime=10080.521703    averageTime=3329.892155 totalComputeTime=179814.176365  numTasks=54
    Machine : node07
        Task::type=''   minTime=702.050721      maxTime=5662.709409     averageTime=3052.297707 totalComputeTime=180085.564685  numTasks=59
    Machine : node08
        Task::type=''   minTime=703.251535      maxTime=5755.713908     averageTime=2803.070356 totalComputeTime=179396.502756  numTasks=64
Finished run successfully.
Total elapsed time = 181923.496816
</pre>
<h3 id="ClusterOverview">Cluster Overview</h3>
<p>The cluster used in this example consists of 9 machines connected using Gigabit Ethernet. The master (root) node contains all applications on its filesystem. The 8 other nodes are identical machines that boot over the network using NFS as their root filesystems. All nodes have NVidia 6 series PCI-Express graphics cards.</p>
<p>All nodes run Debian Lenny. By virtue of using the same NFS mount, all client nodes have identical software installed.</p>
<p>The 8 client nodes have a 500GB disk each. The disks are pooled into one large 4TB filesystem using the excellent <a class="ext-link" href="http://www.gluster.org/"><span class="icon">GlusterFS</span></a>. By mounting the GlusterFS filesystem from all nodes, they all see the exact same data.</p>
<h3>Misc Tips</h3>
<p> Use <strong>GNU screen</strong> to start the <strong>vpbmaster</strong> command. You can then monitor the progress over the network and save the command line output to review later.</p>
<p> <a class="ext-link" href="http://gridengine.sunsource.net/"><span class="icon">Sun Grid Engine</span></a> makes reprojecting hundreds of files in parallel easier. You just submit the whole batch at once using <strong>find -exec</strong> and a simple script.</p>
<p> Since <strong>ssh</strong> to all nodes is set up already, install <strong>dsh</strong> (distributed shell) on the root node. Running a single command on all nodes then become as easy as:</p>
<pre class="wiki">dsh -a "ls /glusterfs"
</pre>
<p>Using GNU/Linux and OpenSceneGraph is fun.</p>
<p>Enjoy.</p>
<p> </p>]]></description>
			<author>jtorresfabra@gmail.com (J.P. Delport)</author>
			<category>Virtual Planet Builder</category>
			<pubDate>Fri, 15 Feb 2013 02:12:03 +0000</pubDate>
		</item>
	</channel>
</rss>
